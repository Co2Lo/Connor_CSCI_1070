{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff939ea-7c5d-40fd-b11a-a901f82599ff",
   "metadata": {},
   "source": [
    "# CHAPTER 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d2b087-e4b8-40d4-b2a1-b837307af050",
   "metadata": {},
   "source": [
    "# Whitespace Formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c76e3e-2c87-41ba-a259-2ffdb3ebe8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python uses identation to start/end code, instead of brackets\n",
    "for i in [1, 2, 3]:\n",
    "    print(i)                  # first line in \"for i\" block\n",
    "    for j in [1, 2, 3]:       \n",
    "        print(j)              # first line in \"for j\" block\n",
    "        print (i+j)           # last line \"for j\" block\n",
    "    print(i)                  # last line \"for i\" block\n",
    "\n",
    "# ...so be VERY careful for code, but inside brackets python ignores it:\n",
    "list = [[1, 2, 3], [4, 5, 6]]\n",
    "easier_to_read_list = [[1, 2, 3],\n",
    "                       [4, 5, 6]]\n",
    "\n",
    "# You can also use backslash to indicate statement continues:\n",
    "hello = 2 + \\\n",
    "        3\n",
    "\n",
    "# Jupyter has a nice \"paste%\" option so pasted-in text formats correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb2fb2-2dec-4dd6-bb8b-58e114a34ba4",
   "metadata": {},
   "source": [
    "# Modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713c65e6-6966-4760-bd0a-ba729c507e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You already saw most of what you need to know, BUT be sure to <import math as math2>\n",
    "# if you've already used \"math\" in code, or if u just wanna make it easier to type.\n",
    "\n",
    "# Also, NEVER\n",
    "<import ___ *>\n",
    "# ...you'll overwrite previous versions of that code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7e35d7-fa1c-40c5-98dc-6a8d726e84fa",
   "metadata": {},
   "source": [
    "# Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a40c7e6-bf50-4287-976a-c0c2a3977907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are \"first-class\" commands that take an input and produce an ouput.\n",
    "# <def> means define, <return> means \"do that\".\n",
    "def double(x)\n",
    "    \"\"You can explain what the function does here, \n",
    "    eg; this one multiples the input by 2.\"\"\n",
    "    return x * 2\n",
    "\n",
    "def apply_to_one(f)\n",
    "    \"Calls the function f with 1 as its argument.\"\n",
    "    return f(1)\n",
    "\n",
    "my_double = double\n",
    "x = apply_to_one(my_double) # is equal to 2.\n",
    "\n",
    "# You can also make short functions, called \"lambdas\".\n",
    "y = apply_to_one(lambda x: x + 4) # is equal to 5.\n",
    "\n",
    "def my_print(message = \"my default message\")\n",
    "    print(message)\n",
    "\n",
    "my_print(\"hello\") # prints 'hello'.\n",
    "my_print( ) # prints 'my default message'.\n",
    "\n",
    "# Basically, whatever you put in that first <def> <return> will be\n",
    "# a placeholder is nothing else is specified.\n",
    "def full_name(first = \"What's his name\", last = \"Something\")\n",
    "    return first + \" \" + last\n",
    "\n",
    "full_name(\"Joel\", \"Grus\") # \"Joel Grus\". Those quotes are from <return>.\n",
    "full_name(\"Joel\") # \"Joel Something\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5bfd63-0e21-43a5-bfa8-a8228d33584c",
   "metadata": {},
   "source": [
    "# Strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83027e9-5b0c-428e-b8e4-49b6dbb001ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These guys can be:\n",
    "single_quoted_string = 'hello'\n",
    "double_quoted_string = \"hello\"\n",
    "\n",
    "# </> is used to encode special characters:\n",
    "tab_string = \"\\t\" # represents tab character.\n",
    "len(tab_string) # is 1.\n",
    "\n",
    "# But if you backslashes to be themselves, use <r\" \"> for raw string.\n",
    "not_tab_string = r\"\\t\" # represents \"/\" and \"t\".\n",
    "len(not_tab_string) # is 2.\n",
    "\n",
    "# If you want to make a multiline string, use \"\"\" \"\"\".\n",
    "\"\"\"First line\n",
    "second line\n",
    "third line.\"\"\"\n",
    "\n",
    "# Use the f-string <f> to combine strings.\n",
    "first_name = \"Joel\"\n",
    "last_name = \"Grus\"\n",
    "full_name = f\"{first_name} {last_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f360270-1b43-4b82-8969-f0713b1350d9",
   "metadata": {},
   "source": [
    "# Exceptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d5cc4-7174-4708-b851-7cfc48d06f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When something goes wrong, Python creates an \"exception\";\n",
    "# unhandled, they'll cause your program to crash, but you can\n",
    "# \"handle\" them with <try> and <except>.\n",
    "\n",
    "try:\n",
    "    print(0 / 0)\n",
    "except: ZeroDivisionError:\n",
    "    print(\"cannot divide by zero\")\n",
    "\n",
    "# Long as you do this, there's no problem in using exceptions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40353855-f643-475b-93d8-7be84d7f1e01",
   "metadata": {},
   "source": [
    "# Lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c1f282-248b-4e7d-84da-eb999b091115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An ordered collection, like a buff array.\n",
    "integer_list = [1, 2, 3]\n",
    "heterogenous_list = [\"string\", 0.1, True]\n",
    "list_of_lists = [integer_list, hetergenous_list, []]\n",
    "\n",
    "list_length = len(integer_list) # equals 3.\n",
    "list_sum = sum(integer_list) # equals 6.\n",
    "\n",
    "# You can also get to nth element with brackets.\n",
    "x = [0, 1, 2, 3, 4, 5]\n",
    "zero = x[0] # equals 0, remember 0-indexed.\n",
    "five = x[-1] # equals 5, 'Pythonic' for last element: wraps around.\n",
    "four = x[-2] # equals 4, 'Pythonic'.\n",
    "\n",
    "# Or you can change values in list.\n",
    "x[0] = -1 # now it's [-1, 1, 2, 3, 4, 5]\n",
    "\n",
    "# You can also slice... remember that the end is non-inclusive (-1)!\n",
    "first_three = x[:3] # [-1, 1, 2]\n",
    "three_to_end = x[3:] # [3, 4, 5]\n",
    "two_to_four = x[2:5] # [2, 3, 4]\n",
    "last_three = x[-3:] # [3, 4, 5]\n",
    "without_first_or_last = x[1:-1] # [1, 2, 3, 4]\n",
    "copy_of_x = x[:] # [-1, 1, ..., 5]\n",
    "\n",
    "# You can also step (skip over).\n",
    "every_third = x[::3] # [-1, 3]\n",
    "four_to_one = x[4:1:-] # [4, 3, 2, 1] ... step every \"1\" don't do nothing\n",
    "\n",
    "# You can check for if something's included in a list with <in>.\n",
    "1 in [1, 2, 3] # True\n",
    "0 in [1, 2, 3] # False\n",
    "\n",
    "# Concatenate lists with <.extend>.\n",
    "x = [1, 2, 3]\n",
    "x.extend([4, 5, 6]) # x is now [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Or, if you want to keep x itself but still combine:\n",
    "y = x + [4, 5, 6] # x is still [1, 2, 3], y is desired.\n",
    "\n",
    "# Unpack a list sequentially, when you know # elements inside...\n",
    "x, y = [1, 2] # x is 1, y is 2\n",
    "\n",
    "# ...or give one a <_> if you don't care about one element.\n",
    "_, x = [1, 2] # y == 2, other can be thrown away."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa51f410-04e0-4876-952b-12acce8b689e",
   "metadata": {},
   "source": [
    "# Tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797bf49-d345-4080-8219-d4b8a15d32cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are immutable lists, with a few caveats.\n",
    "# You can use parenthesis, or nothing:\n",
    "tuple = (1, 2)\n",
    "other_tuple = 1, 2\n",
    "\n",
    "# Useful to return multiple values from a function:\n",
    "def sum_and_product(x, y):\n",
    "    return (x + y), (x * y)\n",
    "\n",
    "sp = sum_and_product(5, 6) # sp is (11, 30)\n",
    "s, p = sum_and_product(5, 6) # s is 11, p is 30\n",
    "\n",
    "# Useful for multiple assignments:\n",
    "x, y = 1, 2\n",
    "x, y = y, x # now x is 2, y is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db66c68f-49e7-49d3-86a2-f0207dc9b47e",
   "metadata": {},
   "source": [
    "# Dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ca1c1-6d30-4ebc-8709-ddfde9dfb5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Links \"values\" and \"keys\" so you can retrieve each quickly.\n",
    "grades = {\"Joel\": 80, \"Tim\": 98}\n",
    "joels_grade = grades[\"Joel\"] # is 80.\n",
    "\n",
    "# Check for key existence using <in>.\n",
    "joel_has_grade = \"Joel\" in grades # True\n",
    "tina_has_grade = \"Tina\" in grades # False\n",
    "\n",
    "# Using <.get> produces default value (0, here) if no key exists.\n",
    "joels_grade = grades.get(\"Joel\", 0) # is 80.\n",
    "tinas_grade = grades.get(\"Kate\", 0) # is 0. \n",
    "no_ones_grade = grades.get(\"No One\") # None.\n",
    "\n",
    "# You can assign key/value pairs with brackets.\n",
    "grades[\"Tim\"] = 45 # replaces old value.\n",
    "grades[\"Tina\"] = 97 # adds third entry.\n",
    "num_students = len(grades) # equals 3.\n",
    "\n",
    "# You can also comb through all keys.\n",
    "tweet = { }\n",
    "    \"user\" : \"joelgrus\",\n",
    "    \"text\" : \"Hello Bro\",\n",
    "    \"retweet_count\" : 2,\n",
    "    \"hastags : [\"greetingsbro\", \"okay\"]\n",
    "# pretend the pointy bracket is where this line begins...\n",
    "# don't know why it won't work. \n",
    "\n",
    "tweet_keys = tweet.keys() # iterable for all keys.\n",
    "tweet_values = tweet.values() # iterable for all values.\n",
    "tweet_items = tweet.items() # iterbals for (key, value) tuples.\n",
    "\n",
    "\"user\" in tweet_keys # True, not Pythonic of you.\n",
    "\"user\" in tweet # Pythonic way of checking for keys.\n",
    "\"joelgrus\" in tweet_values # True (slow, but only way to check). \n",
    "\n",
    "# You can't use lists as keys, instead use a tuple or string.\n",
    "\n",
    "# defaultdict is like reg dict, but when you look up a nonexistent key,\n",
    "# it'll create it and assign it a default value, as opposed to KeyError.\n",
    "from collections import defaultdict\n",
    "\n",
    "word_counts = defaultdict(int) # int() produces 0.\n",
    "for word in document:\n",
    "    word_counts[word] += 1\n",
    "\n",
    "dd_list = defaultdict(list) # list() produces an empty list.\n",
    "dd_list[2].append(1) # now dd_list contains {2: [1]}\n",
    "\n",
    "dd_dict = defaultdict(dict) # dict() produces an empty dict.\n",
    "dd_dict[\"Joel\"][\"City\"] = \"Seattle\" # {\"Joel\" : {\"City\": Seattle\"}}\n",
    "\n",
    "dd_pair = defaultdict(lambda: [0, 0])\n",
    "dd_pair[2][1] = 1 # now dd_pair contains {2: [0,1]}\n",
    "\n",
    "#...very useful when using dictionaries to collect results by-key,\n",
    "# but don't want to see if key exists every time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec05d812-2c5e-4344-b707-26fdcb227559",
   "metadata": {},
   "source": [
    "# Counters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e4cc8c-9f3a-4853-9463-8c78ce8f7aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically a defaultdict which generates the number of times a key\n",
    "# crops up in an element. For example:\n",
    "from collections import Counter\n",
    "c = Counter([0, 1, 2, 0]) # c = {0: 2, 1: 1, 2: 1}\n",
    "\n",
    "# Easiest way to word count:\n",
    "word_counts = Counter(document)\n",
    "\n",
    "# ...or determine most common words, number of times they are.\n",
    "for word, count in word_counts.most_common(10):\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66916521-ab52-427e-89a8-4ba728b953dc",
   "metadata": {},
   "source": [
    "# Sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d18bb6-921e-4cda-9124-9dacb19c61e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A set is like a list with no repeating——distinct——elements.\n",
    "primes_below_10 = {2, 3, 5, 7}\n",
    "\n",
    "# You cannot, however, designte {} as an \"empty set\", since that's\n",
    "# the notation for an empty dict. Instead:\n",
    "s = set()\n",
    "s.add(1) # s = {1}\n",
    "s.add(2) # s = {1, 2}\n",
    "s.add(2) # s = {1, 2} (see above)\n",
    "\n",
    "# You use sets because the <in> command is very slow on lists, but not on sets:\n",
    "stopwords_list = [\"a\", \"an\", \"at\"] + lotsa_others + [\"yet\", \"zeebra\"]\n",
    "\"zip\" in stopwords_list # False, but takes forever (checks every element)\n",
    "\n",
    "stopwords_set = set(stopwords_list)\n",
    "\"zip\" in stopwords_set # False, very fast\n",
    "\n",
    "# Another reason is to reduce a list only to its distinct elements:\n",
    "item_list = [1, 2, 3, 2, 2, 3,]\n",
    "item_set = set(item_list) # {1, 2, 3}\n",
    "distinct_item_list = list(item_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4d1bd3-2fac-4194-96a6-65519ffafa6b",
   "metadata": {},
   "source": [
    "# Control Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad3a86-b211-4fc3-b861-b06498a55f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference in-class notes for if, elif, else statements.\n",
    "\n",
    "# You can also write if-then-else on one line (ternary):\n",
    "parity = \"even\" if x % 2 == 0 else \"odd\"\n",
    "\n",
    "# <While> loop:\n",
    "x = 0\n",
    "while x < 10:\n",
    "    print(f\"{x}is less than 10\"}\n",
    "    x += 1 # Would print \"{num} is less than 10\"\n",
    "\n",
    "# <For> and <in>:\n",
    "for x in range(10):\n",
    "    print(f\"{x} is less than 10\") # Would print same as above\n",
    "\n",
    "# <Continue> allows you to add more if statements:\n",
    "for x in range(10):\n",
    "    if x == 3:\n",
    "        continue # Consider next\n",
    "    if x == 5:\n",
    "        break # Quit loop entirely (it'll stop at 5)\n",
    "    print(x) # Will only print 0, 1, 2, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7103ed3-ee05-4298-94dc-ef7d45a69917",
   "metadata": {},
   "source": [
    "# Truthiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e3a1c41-713e-4828-a160-0223bf66d33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Booleans work as in most languages, except capitalized here:\n",
    "one_is_less_than_two = 1 < 2 # True\n",
    "true_equals_false = True == False # False\n",
    "\n",
    "# <None> indicates nonexistent value, much like \"null\":\n",
    "x = None\n",
    "assert x == None # Not very Pythonic of you\n",
    "assert x is None # Pythonic of you\n",
    "\n",
    "# Python lets you use any value that it can judge with Bool, \n",
    "# all of the following are \"falsy\":\n",
    "falsies = [False, None, [], {}, \"\", set(), 0, 0.0]\n",
    "\n",
    "# ...everything else is treated as \"True\", meaning you can use\n",
    "# <if> statements on empty lists, strings, dictionaries.\n",
    "s = function_returning_blank_string()\n",
    "if s:\n",
    "    first_character = s[0] # The first character is equal to index 0...\n",
    "else:\n",
    "    first_character = \"\" # ...otherwise it's false.\n",
    "\n",
    "# ...alternatively:\n",
    "first_char = s and s[0]\n",
    "\n",
    "# ...given <and> returns second value when truthy and first when not.\n",
    "# Similarly, if x is either a number or \"none\":\n",
    "safe_x = x or 0 # Number assured\n",
    "safe_x = x if x is not None, else 0 # Same thing\n",
    "\n",
    "# And finally, the <all> function takes an iterable and returns \"True\" when\n",
    "# every element is truthy. An <any> function returns \"True\" when at least one is.\n",
    "all([True, 1, {3}]) # True, all truthy\n",
    "all([True, 1, {}]) # False, {} falsy\n",
    "any([True, 1, {}]) # True, True truthy, as is 1\n",
    "all([]) # True, no falsy elements in list\n",
    "any([]) # False, no truthy elements in list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482dc26-f91a-4913-a66d-ce5df4145edd",
   "metadata": {},
   "source": [
    "# Sorting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94024239-77b2-49ff-8e95-d26c640dc93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple:\n",
    "x = [4, 1, 2, 3]\n",
    "y = sorted(x) # y is [1, 2, 3, 4], x unchanged\n",
    "x.sort() # now x is [1, 2, 3, 4]\n",
    "\n",
    "# Or, sort by absolute value with <abs>, largest -> smallest with <reverse=True>:\n",
    "x = sorted([-4, 1, -2, 3], key=abs, reverse=True) # [-4, 3, -2, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24971618-0238-4a31-99ed-02e3e1bbd499",
   "metadata": {},
   "source": [
    "# List Comprehensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac7a049-29fe-4960-96a6-48fb5b743eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming lists——making new lists of only certain elements of others\n",
    "# the Pythonic way (the Pythonic way):\n",
    "even_numbers = [x for x in range(5) if x % 2 == 0] # [0, 2, 4]\n",
    "squares = [x * x for x in range(5)] # [0, 1, 4, 9, 16]\n",
    "even_squares = [x * x for x in even_numbers] # [0, 4, 16]\n",
    "\n",
    "# Dict and set transformations (the Pythonic way):\n",
    "square_dict = {x: x * x for x in range(5)} # {0: 0, 1: 1, 2: 4, 3: 9, 4: 16}\n",
    "square_set = {x * x for x in [1, -1]} # {1} ... note it's transforming a list\n",
    "\n",
    "# <_> if you don't want it to spit out the value:\n",
    "zeros = [0 for _ in even_numbers]\n",
    "\n",
    "# You can also use multiple <fors>:\n",
    "pairs = [(x, y)\n",
    "         for x in range(10)\n",
    "         for y in range(10)] # 100 pairs (0,0) (0,1) ... (9, 8), (9, 9)\n",
    "\n",
    "# ...and <fors> can piggyback off one another:\n",
    "increasing_pairs = [(x, y)                      # x will always < y\n",
    "                    for x in range(10)          # list of nums in range(low, high)\n",
    "                    for y in range(x + 1, 10)]  # equals [lowest, lowest + 1, ...,\n",
    "                                                # highest - 2, highest - 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7cd3d1-f8d0-471c-9802-a0e9e0cac001",
   "metadata": {},
   "source": [
    "# Automated Testing and Assert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed657077-26a1-4d94-8500-57d3ad135bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <assert> statements cause your code to raise an \"AssertionError\" if chosen\n",
    "# condition is not truthy:\n",
    "assert 1 + 1 == 2\n",
    "assert 1 + 1 == 2, \"1 + 1 equals 2, didn't here\" # Your message there\n",
    "\n",
    "# Use, use, use <assert> to confirm that functions in your code are correct.\n",
    "def smallest_item(xs):\n",
    "    return min(xs)\n",
    "\n",
    "assert smallest_item([10, 20, 5, 40]) == 5\n",
    "\n",
    "# or, to <assert> about an input to a function:\n",
    "\n",
    "def smallest_item(xs):\n",
    "    assert xs, \"empty list has no smallest item\"\n",
    "    return min(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff87073-905f-4ff3-8318-067dad5880df",
   "metadata": {},
   "source": [
    "# Object-Oriented Programming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3667ef-a480-45b1-b94e-b60d19b36e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes encapsulate data and the functions that operate on them.\n",
    "# Let's explain it with a \"counting clicker\", like the one they use at Grand\n",
    "# while the system is down, to count the number of people swiping in.\n",
    "\n",
    "# Our clicker will maintain a \"count\", can be \"clicked\", you can \"read_count\",\n",
    "# and it can be \"reset\" back to zero.\n",
    "\n",
    "# To define a class (there has to be a capital before every word!!):\n",
    "class CountingClicker:\n",
    "    \"\"\"A class should have a docstring explaining it, like a function.\"\"\"\n",
    "\n",
    "# A class contains \"member\" functions within it, each one \n",
    "# takes the first parameter \"self\", which refers to its \n",
    "# instance——existence at a time.\n",
    "\n",
    "# A \"constructor\" <_init_> takes whatever parameters you need to construct these\n",
    "# instances and sets them up. This <_init_> \"method name\" is also called a \"dunder\"\n",
    "# method, featuring special behaviors.\n",
    "\n",
    "def _init_(self, count = 0):\n",
    "    self.count = count         # Now, each count has an instance.\n",
    "\n",
    "# These instances occur naturally whenever you use the class name, eg;\n",
    "clicker1 = CountingClicker() # Intance when initialized to 0\n",
    "clicker2 = CountingClicker(100) # ...when count = 100\n",
    "clicker3 = CountingClicker(count = 200) # (same as above, just more explicit)\n",
    "\n",
    "# Another method is <__repr__>, which creates the string representation\n",
    "# of a class instance, as seen below:\n",
    "def __repr__(self):\n",
    "    return f\"CountingClicker(count={self.count})\"\n",
    "\n",
    "# And finally, we need to make the public application progrmaming interface (API):\n",
    "\n",
    "def click(self, num_times = 1):\n",
    "    \"\"\"Click the clicker some number of times.\"\"\"\n",
    "    self.count += num_times\n",
    "\n",
    "def read(self):\n",
    "    return self.count\n",
    "\n",
    "def reset(self):\n",
    "    self.count = 0\n",
    "\n",
    "# He asserts it, I trust him.\n",
    "\n",
    "# You can now create \"subclasses\" that inherit functionality from parent class.\n",
    "# For example, you could create a non-resettable clicker:\n",
    "class NoResetClicker(CountingClicker):\n",
    "    def reset(self):\n",
    "        pass           # Now the reset method does nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83be402-c77d-464e-bb02-3e3a8c59806d",
   "metadata": {},
   "source": [
    "# Iterables and Generators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157daae2-ff4d-459d-ba43-c9e79e5189c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I should've prefaced this with the fact an iterable is literally just anything\n",
    "# that loops and/or can be \"iterated\" using the <for> function.\n",
    "\n",
    "# Instead of making a list of a billion digits (sm space) to use at your will,\n",
    "# just use a generator to produce those you need——with all of a list's benefits.\n",
    "# One way you could use them is through the <yield> operator:\n",
    "def generate_range(n):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        yield i  # Every call to yield produces a value of the generator\n",
    "        i += 1\n",
    "\n",
    "# Then make a loop which will take in the yielded values one-by-one until \n",
    "# none are left:\n",
    "for i in generate_range(10):\n",
    "    print(f\"i: {i}\")\n",
    "\n",
    "# A second way to create generators is using <for> wrapped in ( ):\n",
    "evens_below_20 = (i for i in generate_range(20) if i % 2 == 0)\n",
    "\n",
    "# Again, the benefit being that if doesn't actually create a shit ton of \n",
    "# numbers which will waste space until you iterate over it (tell it to \"go\");\n",
    "# this has the added benefit of use in complicated data-processing.\n",
    "data = natural_numbers()\n",
    "evens = (x for x in data if x % 2 == 0)\n",
    "even_squares = (x ** 2 for x in evens)\n",
    "even_squares_ending_in_six = (x for x in even_squares if x % 10 == 6)\n",
    "\n",
    "# Often, this'll be combined with the <enumerate> function:\n",
    "names = {\"John\", \"Quincy\", \"Darnell\", \"Stacy\"}\n",
    "for i, name in enumerate(names):\n",
    "    print(f\"name {i} is {name})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378dc3f4-1949-4e44-91c4-962c2bea9a19",
   "metadata": {},
   "source": [
    "# Randomness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b8fbaa-6972-4c86-a337-fd3447f1baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate random numbers:\n",
    "import random\n",
    "random.seed(10) # For same results every time, remember stats.\n",
    "\n",
    "four_uniform_randoms = [random.random() for _ in range(4)] # Again, uniform\n",
    "# simply meaning any other number was possible, no need to get confused.\n",
    "\n",
    "# You can combine this with ranges:\n",
    "random.randrange(10) # Chooses randomly from 1-->10\n",
    "\n",
    "# Or shuffle a list:\n",
    "list_to_ten = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "random.shuffle(list_to_ten)\n",
    "print(list_to_ten)\n",
    "\n",
    "# Or pick one element at random from list:\n",
    "favclass = random.choice[\"Taming Big Data\", \"Baming Dig Tata\", \"Daming Tig Bata\"]\n",
    "\n",
    "# Or sample from a list without duplication:\n",
    "lottery_numbers = range(60)\n",
    "winning_numbers = random.sample(lottery_numbers, 2)\n",
    "\n",
    "# Or with duplicates (think):\n",
    "with_dupes = [random.choice(range(10)) for _ in range(4)] # Could be overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835f9e16-c585-4eb2-a56b-25d715d70b58",
   "metadata": {},
   "source": [
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca14a135-77d3-406b-b802-c975356137ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <re> expressions allow you to search text:\n",
    "import re\n",
    "examples_of_re = [ # Bc they're true...\n",
    "    not re.match(\"a\", \"cat\"), # \"cat\" doesn't start with \"a\"\n",
    "    re.search(\"a\", \"cat\"), # \"cat\" does have \"a\" in it\n",
    "    not re.search(\"c\", \"dog\"), # \"dog\" doesn't have \"c\" in it\n",
    "    3 == len(re.split(\"[ab]\", \"carbs\")), # Split on a or b to c r s\n",
    "    \"R-D-\" == re.sub(\"[0-9]\", \"-\", \"R2D2\") # Replace digits with dashes\n",
    "]\n",
    "assert all(re_examples), \"all the regex examples should be True\" # ...bc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a9167-8d75-461e-8f3c-277256d7013a",
   "metadata": {},
   "source": [
    "# zip and Argument Unpacking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a71b09-0d87-4f33-aa3d-44ba7b22f801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can <zip> 2+ iterables together, creating one iterable of tuples:\n",
    "list1 = ['a', 'b', 'c']\n",
    "list2 = [1, 2, 3]\n",
    "\n",
    "[pair for pair in zip(list1, list2)] # ...creates [('a', 1), ('b', 2), ('c', 3)]\n",
    "\n",
    "# If you <zip> together two lists of varying length, it just stops when short ends.\n",
    "# You can \"unzip\"——argument unpack——like so:\n",
    "pairs = [('a', 1), ('b', 2), ('c', 3)]\n",
    "letters, numbers = zip(*pairs)\n",
    "\n",
    "# You can also do a lil quirky thing where you combine the unzipped:\n",
    "add(*[1, 2]) # Equals 3!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7316973b-0a4f-4edb-94a0-881ed0384757",
   "metadata": {},
   "source": [
    "# args and kwargs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361652c5-023e-434c-91fb-b6bf9bf3c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes a little bit of sense, but not a lot. Read it now, try again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc821f-d4e3-4f27-81b5-1a608cefe4df",
   "metadata": {},
   "source": [
    "# Type Annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f228eb2c-087b-4201-b4a7-019a94722c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do them. (Genuinely fill out this section and the above if you more time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78322305-dc47-4c54-9fff-05277d6be9d8",
   "metadata": {},
   "source": [
    "# CHAPTER 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e68a4c0-232d-4f5c-aee8-440e1be375ec",
   "metadata": {},
   "source": [
    "# matplot lib (Plots):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91958364-7cf3-4bf8-80f0-b386b7711295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (is used to create plots, charts, and visualize data generally)\n",
    "\n",
    "from matplotlib import pyplot at plt\n",
    "\n",
    "years = [1950, 1960, 1970, 1980, 1990]\n",
    "gdp = [300.2, 300.3, 567.2, 4993.5, 9320.9]\n",
    "\n",
    "# Create a line chart:\n",
    "plt.plot(years, gdp, color = 'green', marker = 'o', linestyle = 'solid')\n",
    "\n",
    "# Add a title:\n",
    "plt.title(\"Nominal GDP\")\n",
    "\n",
    "# Add a label to the y-axis:\n",
    "plt.ylabel(\"Billions of $\")\n",
    "plt.show() # Generates an upward-sloping, Nominal GDP graph with green line/points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b56402-57e0-409c-bce1-f24b064a1232",
   "metadata": {},
   "source": [
    "# Bar Charts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b94e32-e284-40af-8bc8-f00bfb9bc5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = [\"Annie Hall\", \"Ben-Hur\", \"Casablanca\", \"Gandhi\"]\n",
    "num_oscars = [5, 11, 3, 8]\n",
    "\n",
    "# Plot bars with x-coordinates [0, 1, 2, 3, 4] and y's [num_oscars]\n",
    "plt.bar(range(len(movies)), num_oscars)\n",
    "\n",
    "plt.title(\"My Favorite Movies\") # Add a title!\n",
    "plt.ylabel(\"Number Academy Awards\") # Label the y-axis!\n",
    "plt.xticks(range(len(movies)), movies) # Label x-axis with movie names at \n",
    "                                       # bar centers.\n",
    "\n",
    "plt.show() # Displays a beautiful bar graph of each movie's Academy Awards.\n",
    "\n",
    "# You can also create histograms, dropping numbers into \"buckets\":\n",
    "\n",
    "from collections import Counter\n",
    "grades = [83, 95, 91, 87, ..., 21]\n",
    "\n",
    "# Bucket grades (the variable, no confusion) by decile, but put 100 in with 90s:\n",
    "histogram = Counter(min(grade // 10 * 10, 90) for grade in grades)\n",
    "plt.bar([x + 5 for x in histogram.keys()] # Shift bars right by 5.\n",
    "        histogram.values(), # Give each bar its appropriate height.\n",
    "        10, # Give each bar a width of 10.\n",
    "        edgecolor = (0, 0, 0)) # Black edges for each bar, for discernability.\n",
    "\n",
    "plt.axis([-5, 105, 0, 5]) # x-axis from -5 to 105... give it a little space.\n",
    "\n",
    "# ...and do all your labels as you did before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbcd6b5-d381-4928-8ab8-99ad38efbba1",
   "metadata": {},
   "source": [
    "# Line Charts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17060fb4-b10b-40f7-a3db-b546c17be9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good for showing trends:\n",
    "varaince = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "bias_squared = [256, 128, 32, 16, 8, 4, 2, 1]\n",
    "total_error = [x + y for x, y in zip(variance, bias_squared)]\n",
    "xs = [i for i, _ in enumerate(variance)]\n",
    "\n",
    "# Use the [heck] out of plt.plot to show multiple series on one chart:\n",
    "plt.plot(xs, variance, 'g-', label = 'variance') # Green solid line.\n",
    "plt.plot(xs, bias_squared, 'r-.', label = 'bias^2') # Green dashed line.\n",
    "plt.plot(xs, total_error, 'b:', label = 'total error') # Green dotted line.\n",
    "\n",
    "# And as a reward for assigning labels to each series, we get a free legend.\n",
    "plt.legend(loc = 9) # Means \"top center\"\n",
    "plt.xticks([]) # For nothing.\n",
    "\n",
    "# ...and the rest is history—— a pretty line chart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c1ace0-d3cb-4711-83db-98369b4a30eb",
   "metadata": {},
   "source": [
    "# Scatterplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f4531-8215-4c2e-8f72-6d2d56d2fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...are the right choice for visualizing relationship b/t two related datasets:\n",
    "\n",
    "friends = [140, 8, 100, 130, 40, 20] # IRLs...\n",
    "minutes = [0, 150, 10, 5, 60, 110] # ...spent on LOL, of course.\n",
    "plt.scatter(friends, minutes)\n",
    "\n",
    "# Label each and every point:\n",
    "for label, friend_count, minute_count in zip(labels, friends, minutes):\n",
    "    plt.annotate(label,\n",
    "        xy = (friend_count, minute_count), # Puts each label with its point.\n",
    "        xytext = (5, -5),                  # But slightly offsets it.\n",
    "        textcoords = 'offset points')      # Cursor over it, would read:\n",
    "\n",
    "# Make sure to always have equal axes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7490b6-5ec9-49a3-ac52-83c8533980eb",
   "metadata": {},
   "source": [
    "# CHAPTER 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b268ed20-f05f-4621-a143-33df7ac9a276",
   "metadata": {},
   "source": [
    "# Describing a Single Set of Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e863afb4-501a-46e2-a2bc-02d808362fd1",
   "metadata": {},
   "source": [
    "Central Tendencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f01e6-39e1-468d-a1f9-849b12b79990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extremes:\n",
    "\n",
    "maximum = max(num_friends)\n",
    "minimum = min(num_friends)\n",
    "\n",
    "def mean(xs: List[float]) -> float: # Mean is equal to...\n",
    "    return sum(xs) / len(xs) # ...sum of all x's / # all x's.\n",
    "\n",
    "mean(num_friends) # Returns 7.33 repeating.\n",
    "\n",
    "# Median:\n",
    "\n",
    "def _median_odd(xs: List[float]) -> float:\n",
    "    \"\"\"If len(xs) is odd, median is middle element.\"\"\"\n",
    "    return sorted(xs)[len(xs) // 2]\n",
    "\n",
    "def _median_even(xs: List[float]) -> float:\n",
    "    \"\"\"If len(xs) is even, it's the average of middle two elements.\"\"\"\n",
    "    sorted_xs = sorted(xs)\n",
    "    hi_midpoint = len(xs) // 2 # eg; length of 4 returns a hi-mid of 2\n",
    "    return (sorted_xs[hi_midpoint - 1] + sorted_xs[hi_midpoint]) / 2\n",
    "\n",
    "def median(v: List[float]) -> float:\n",
    "    \"\"\"Finds the 'middle-most' value of v.\"\"\"\n",
    "    return _median_even(v) if len(v) % 2 == 0 else _median_odd(v)\n",
    "\n",
    "# Quartiles:\n",
    "\n",
    "def quartile(xs: List[float], p: float) -> float:\n",
    "    \"\"\"Returns the pth-percentile value for x.\"\"\"\n",
    "    p_index = int(n * len(xs)) # For example, plugging in 0.1 would return\n",
    "    return sorted(xs)[p_index] # 1 for our list, because it's a tenth of way up.\n",
    "\n",
    "# Mode:\n",
    "\n",
    "def mode(x: List[float]) -> List[float]:\n",
    "    \"\"\"Returns a list, since there could be more than one 'most' common.\"\"\"\n",
    "    counts = Counter(x)\n",
    "    max_count = max(counts.values())\n",
    "    return [x_i for x_i, count in counts.items()\n",
    "            if count == max_count]\n",
    "\n",
    "assert set(mode(num_friends)) == {1, 6} # Just for kicks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36cccf8-1241-4a11-9eae-8676c52290b4",
   "metadata": {},
   "source": [
    "Dispersion (measure of data spread):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75a59c-0464-4015-86c6-19ea32b1d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range, measures through max - min, 0 being the tightest:\n",
    "\n",
    "def data_range(xs: List[float]) -> float:\n",
    "    return max(xs) - min(xs)\n",
    "\n",
    "# Variance, the more refined range: \n",
    "\n",
    "def de_mean(xs: List[float]) -> List[float]:\n",
    "    \"\"\"Subtract mean of x's from each x, so mean now becomes 0.\"\"\"\n",
    "    x_bar = mean(xs) # Remember AP stats symbol.\n",
    "    return [x - x_bar for x in xs]\n",
    "\n",
    "def variance(xs: List[float]) -> float:\n",
    "    \"\"\"(Deviation from mean)^2 ... average of all of them.\"\"\"\n",
    "    assert len(xs) >= 2, \"Variance requires at LEAST two x's.\"\n",
    "\n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    return sum_of_squares(deviations) / (n - 1)\n",
    "\n",
    "assert 81.54 < variance(num_friends) < 81.55\n",
    "\n",
    "# And because standard deviation is the best measure of any:\n",
    "\n",
    "import math\n",
    "def standard_deviation(xs: List[float]) -> float:\n",
    "    \"\"\"The standard deviation is the square root of the variance.\"\"\"\n",
    "    return math.sqrt(vairance(xs))\n",
    "\n",
    "# And because standard deviation is flawed, considering outliers:\n",
    "\n",
    "def interquartile_range(xs: List[float]) -> float:\n",
    "    \"\"\"Returns the difference between the 75%-ile and the 25%-ile.\"\"\"\n",
    "    return quantile(xs, 0.75) - quantile(xs, 0.25)\n",
    "\n",
    "# Which chops outliers, and probably gives you something more reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b176021-81bb-441c-bf64-a2a2ac89535a",
   "metadata": {},
   "source": [
    "# Correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f751fda-f320-4137-9f8d-e5bafbd0e712",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2257367906.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    def covariance(xs: List[float], ys List[float]) -> float:\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Covariance, the bastard son:\n",
    "\n",
    "def covariance(xs: List[float], ys List[float]) -> float:\n",
    "    assert len(xs) == len(ys) \"Gotta have same number of x's and y's.\"\n",
    "    return dot(de_mean(xs), de_mean(ys)) / (len(xs) - 1)\n",
    "\n",
    "# Measures how two variables vary from their means, together. In other\n",
    "# words, when stock x moves alongside stock y, positive covariance, when\n",
    "# they move opposite ways, negative. When unrelated, zero.\n",
    "\n",
    "# This is pretty good, but creates the illusion of larger covariance if\n",
    "# everyone's values were multiplied by two, say.\n",
    "\n",
    "# Correlation fixes this by dividing out the standard deviation:\n",
    "\n",
    "def correlation(xs: List[float], ys: List[float]) -> float:\n",
    "    \"\"\"Measures how much xs and ys vary in tandem about their means.\"\"\"\n",
    "    stdev_x = standard_deviation(xs)\n",
    "    xtdev_y = standard_deviation(ys)\n",
    "    if stdev_x > 0 and stdev_y > 0:\n",
    "        return covariance(xs, ys) / stdev_x / stdev_y\n",
    "    else:\n",
    "        return 0 # If there's no variation, the correlation is zero.\n",
    "\n",
    "# Wait! Kill the outliers lest they kill your data!\n",
    "\n",
    "outlier = num_friends.index(100) # Index\n",
    "num_friends_cleaned = [x\n",
    "                       for i, x in enumerate(num_friends)\n",
    "                       if i != outlier] # If you remove the outlier.\n",
    "\n",
    "daily_minutes_cleaned = [x\n",
    "                         for i, x in enumerate(daily_minutes)\n",
    "                         if i != outlier]\n",
    "\n",
    "daily_hours_cleaned = [dm / 60 for dm in daily_minutes_cleaned]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adafe5e-91e9-474b-afb2-b06c68ee6bee",
   "metadata": {},
   "source": [
    "\"Simpson's Paradox\" simply means, check your data for confounding variables / design your experiment properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1613bd-a091-44e7-8c30-357168253edf",
   "metadata": {},
   "source": [
    "\"Correlational Caveats\" include things you wouldn't otherwise know through the above methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e04f9-826d-4835-9229-2269ffbc73b2",
   "metadata": {},
   "source": [
    "# CHAPTER 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61380441-ee39-4378-b080-bdb0597451c1",
   "metadata": {},
   "source": [
    "# Dependence and Indendence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61310c2-c46b-422c-8b61-6ced3d03c9b8",
   "metadata": {},
   "source": [
    "Independent: P(E, F) = P(E)P(F) ... the chance of both happening is simply chance E happens multiplied by chance F happens.\n",
    "\n",
    "Dependent: P(E|F) = P(E, F)/P(F)       OR       P(E, F) = P(E|F)P(F) ... the chance E happens, given we KNOW F happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a17690d-9a75-4d9a-8d84-d221f5502008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the example of predicting boy/girl:\n",
    "\n",
    "import enum, random # Enum is a typed set of enumerated values.\n",
    "class Kid(enum.Enum):\n",
    "    BOY = 0\n",
    "    GIRL = 1\n",
    "\n",
    "def random_kid() -> Kid:\n",
    "    return random.choice([Kid.BOY, Kid.GIRL])\n",
    "\n",
    "both_girls = 0\n",
    "older_girl = 0\n",
    "either_girl = 0\n",
    "\n",
    "random.seed(0) # The nostalgia.\n",
    "\n",
    "for _ in range(10000):\n",
    "    younger = random_kid()\n",
    "    older = random_kid()\n",
    "    if older == Kid.GIRL:\n",
    "        older_girl += 1\n",
    "    if older == Kid.GIRL and younger == Kid.GIRL:\n",
    "        both_girls += 1\n",
    "    if older == Kid.GIRL or younger == Kid.GIRL:\n",
    "        either_girl += 1\n",
    "\n",
    "print(\"P(both | older):\", both_girls / older_girl) # 0.514 ~ 1/2\n",
    "print(\"P(both | either):\", both_girls / either_girl) # 0.342 ~ 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2531b029-3f3c-42fb-b0cd-692f2d3e915d",
   "metadata": {},
   "source": [
    "# Bayes's Theorem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cefa0b4a-1f0e-4962-84d3-e4d245201428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search it up, I'm not understanding it the way the book talks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa759ce0-44dc-4e29-b7fd-b0a2db78c566",
   "metadata": {},
   "source": [
    "# Random Variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6febb99-4332-4405-bcab-0b453bcfd2a8",
   "metadata": {},
   "source": [
    "Any variable whose possible values can be mapped into a probability distribution. For example, a random variable set to = 1 if coin flip is heads, and set to = 0 if tails. Distribution would equal 0 with probability 0.5, and 1 with probability 0.5.\n",
    "\n",
    "A range(10) would have a distribution with probability 0.1 for all numbers 0-9.\n",
    "\n",
    "Thusly, an \"expected value\" would be the average of all probabilities, eg; coin flip = (0 * 1/2) + (1 * 1/2)... range(10) = 4.5 (0 * 0.1 + 1 * 0.1 ..., 9 * 0.1).\n",
    "\n",
    "Always remember, you can \"condition\" your variables (change their probabilities)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a629db-0588-4675-a55d-fb382f7c837c",
   "metadata": {},
   "source": [
    "# Continuous Distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e888d3ed-f80d-488c-87eb-d09d7a6f5c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Discrete distribution\" = variable can only take on certain values,\n",
    "# \"probability density function\" measures chances var is b/t two vals:\n",
    "def uniform_pdf(x: float) -> float:\n",
    "    return 1 if 0 <= x < 1 else 0\n",
    "\n",
    "# A \"cumulative distribution function\" gives probability random var\n",
    "# is less than or equal to certain value:\n",
    "def uniform_cdf(x: float) -> float:\n",
    "    \"\"\"Returns probability a uniform random variable is < = x.\"\"\"\n",
    "    if x < 0: return 0 # Uniform random is never less than 0.\n",
    "    elif x  1: return x # ex; P(X <= 0.4) = 0.4\n",
    "    else: return 1 # ...but always less than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6498f6d-d311-429c-8b40-3a216ecf21f9",
   "metadata": {},
   "source": [
    "# Normal Distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7561203a-e7a9-4b4b-aadc-aa455291a6e8",
   "metadata": {},
   "source": [
    "Normal Distribution is classic bell-shaped curve you know and love, with Mew = 0 and stdev = 1 (producing the equations X = stdev(Z) + Mew ... for stedev norm var \"Z\", OR Z = (X - Mew)/stdev ... for norm var \"X\")\n",
    "\n",
    "This (re-)introduces the concept of \"invert_norm_cdf\", which produces the value of a specific probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6aceb2-05ba-418c-b82d-afacfa9b2d7f",
   "metadata": {},
   "source": [
    "# The Central Limit Theorem:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c87998-726d-4bae-98f1-87259bfc2e0d",
   "metadata": {},
   "source": [
    "Essentially states that the prob dist of the mean will be normal, given the size of the sample is large enough.\n",
    "\n",
    "Yes, there are other formulae in here, but nothing very expressely related to the coding I think you'll be doing.\n",
    "\n",
    "...(just know how to use cdf, pdf, and their inverses)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f54f63",
   "metadata": {},
   "source": [
    "# CHAPTER 7:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0963535",
   "metadata": {},
   "source": [
    "Dazed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "641590b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember nulls? Those H_0 : p = 0.5 vs H_1 : p =/ 0.5 from stats?\n",
    "from typing import Tuple\n",
    "import math\n",
    "\n",
    "def normal_approximation_to_binomial(n: int, p: float) -> Tuple[float, float]:\n",
    "    \"\"\"Returns mu and sigma corresponding to a Binomial(n, p)\"\"\"\n",
    "    mu = p * n\n",
    "    sigma = math.sqrt(p * (1 - p) * n)\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a74f30",
   "metadata": {},
   "source": [
    "# CHAPTER 8:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee91cf82",
   "metadata": {},
   "source": [
    "Let's say you have an array of numbers (a vector). Let's say you also have a function f(x) = x^2. To find out at what point in the vector the function is increasing the most——to \"maximize it——you'd use a derivative. However, you can also use the less accurate \"difference quotient\", which is essentially a line between two points, and the closer those two points are together, the closer it gets to the actual tangent line, whose slope we need:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a339461",
   "metadata": {},
   "source": [
    "Ender Eye: The ender eye acts like the gradient in gradient descent. It shows you the direction of the nearest stronghold. If you throw the ender eye and it moves rapidly in a certain direction, it means the stronghold is in that direction, and you should head that way.\n",
    "\n",
    "Step Size: The step size in gradient descent is like the distance you travel each time you throw the ender eye. If the ender eye moves a lot, you take a big step. If it moves only a little, you take a small step. This helps you navigate efficiently towards the stronghold.\n",
    "\n",
    "Finding the Stronghold: You keep throwing the ender eye, adjusting your direction and step size based on its movement, until you eventually reach the stronghold. The larger the movement of the ender eye, the more confident you are that you're heading in the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e23dc439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "def difference_quotient(f, x, h,):\n",
    "    return(f(x + h) - f(x)) / h\n",
    "\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "def derivative(x):\n",
    "    return 2 * x # You'd have to manually calculate this, and plug it in for\n",
    "                 # every damn point. Instead, we'll use the difference_quotient\n",
    "                 # estimation, increasing by a step of h = 0.001 in a direction.\n",
    "\n",
    "xs = range(-10, 11)\n",
    "actuals = [derivative(x) for x in xs]\n",
    "estimates = [difference_quotient(square, x, h = 0.001) for x in xs]\n",
    "\n",
    "# Which would return the fact that difference quotients get pretty damn\n",
    "# close to the derivatives at each point (the f'(x) equations overlap)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2697ba4",
   "metadata": {},
   "source": [
    "Multivariable Functions? Not to worry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc9a7ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But what if you had multiple variables? Just hold the other variables as fixed\n",
    "# and treate the \"nth\" partial derivative as a function... I guess:\n",
    "\n",
    "def partial_difference_quotient(f, v, i, h):\n",
    "    \"\"\"Returns the nth partial difference quotient of f, at v.\"\"\"\n",
    "    w = [v_j + (h if j == i else 0)\n",
    "         for j, v_j in enumerate(v)] # Add h to ONLY the nth element of v.\n",
    "    \n",
    "    return (f(w)-f(v)) / h\n",
    "\n",
    "def estimate_gradient(f, v, h):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i in range(len(v))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462556c2",
   "metadata": {},
   "source": [
    "Estimating the Gradient (Continued):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4ec33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But let's imagine we had no idea what the function was, or the fact\n",
    "# its f'(x) moves linearlly. We'd start a random point and just go:\n",
    "\n",
    "import random\n",
    "# from scratch.linear_algebra import distance, add, scalar_multiply\n",
    "\n",
    "def gradient_step(v):\n",
    "    assert len(v) = len(gradient)\n",
    "    step = scalar.multiply(step_size, gradient)\n",
    "    return add(v, step) # Move up by a little.\n",
    "\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "# Pick random starting point:\n",
    "v = [random.uniform(-10, 10) for i in range(3)]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    grad = sum_of_squares_gradient(v)\n",
    "    v = gradient_step(v, grad, -0.01) # I don't know, man.\n",
    "    print(epoch, v)\n",
    "\n",
    "assert distance(v, [0, 0, 0]) < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a195c2d",
   "metadata": {},
   "source": [
    "Using the Gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc64a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's say you have a vector whose data moves in a pattern, but you\n",
    "# don't the function——model——that best fits that pattern. You'd use a\n",
    "# gradient to find how accurate estimates vs. actual data is, and adjust\n",
    "# accordingly:\n",
    "\n",
    "inputs = [(x, 20 * x + 5) for x in range(-50, 50)] # X, Y = 20x + 5\n",
    "def linear_gradient(x, y):\n",
    "    slope, intercept = theta\n",
    "    predicted = slope * x + intercept # A prediction for the function.\n",
    "    error = (predicted - y)           # Error equation.\n",
    "    squared_error = error ** 2        # Minimize squared error...\n",
    "    grad = [2 * error * x, 2 * error] # Gradient is however much off.\n",
    "    return grad                       # It's so big because that way\n",
    "                                      # big errors are even more\n",
    "    # egrigous, and the smaller they get, the more correct exponentially.\n",
    "    # And if the error is negative, the change is a lot less worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303b1845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the whole dataset——that was just one point lmao——you'll need to:\n",
    "\n",
    "# 1.) Start with random values for slope and intercept:\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(5000): # (Epoch simply a lot of iterations).\n",
    "    # 2.) Compute the mean of the gradients.\n",
    "    grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "    # 3.) Take a step in that direction.\n",
    "    theta = gradient_step(theta, grad, -learning_rate)\n",
    "    print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1, \"slope should be about 20\"\n",
    "assert 4.9 < intercept < 5.1, \"intercept should be less than 5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be115fb",
   "metadata": {},
   "source": [
    "Minibatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8987d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As seems to be the pattern in this book, here's the better way of going\n",
    "# about doing this... because the above method calculates it less\n",
    "# effeciently, especially with big data sets:\n",
    "\n",
    "from typing import TypeVar, List, Iterator\n",
    "\n",
    "T = TypeVar('T') # This allows you to create \"generic\" functions\n",
    "\n",
    "def minibatches(dataset, batch_size, shuffle):\n",
    "    \"\"\"Generates 'batch_size'-sized minibatches from the dataset.\"\"\"\n",
    "    # \"Start\" indexes from 0, batch_size, 2 * batch_size\n",
    "    batch_starts = [start for start in range(0, len(dataset)), batch_size]\n",
    "    if shuffle: random.shuffle(batch_starts) # Shuffle the batches.\n",
    "    for start in batch_starts:\n",
    "        end = start + batch_size\n",
    "        yield dataset[start:end] # Wtf.\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for batch in minibatches(inputs, batch_size = 20)\n",
    "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    print(epoch, theta) # I'm going to assume this does the same gradient-\n",
    "                        # averaging shit as the above code, except in terms\n",
    "                        # of increasingly larger batches.\n",
    "    \n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1, \"slope should be about 20\"\n",
    "assert 4.9 < intercept < 5.1, \"intercept should be less than 5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49abaea5",
   "metadata": {},
   "source": [
    "Stochastic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e33181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also take steps based on one training example at a time:\n",
    "\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1, \"slope should be about 20\"\n",
    "assert 4.9 < intercept < 5.1, \"intercept should be less than 5\"\n",
    "\n",
    "# I genuinely have no idea how this is any different than before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d71bf",
   "metadata": {},
   "source": [
    "# CHAPTER 9:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea7d63a-5a55-4f47-a5b5-fe26498af44e",
   "metadata": {},
   "source": [
    "# stdin and stdout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a929ec8-4d42-428a-9872-8375fceec5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can streamline data through <sys.stdin> and <sys.stdout>, for example,\n",
    "# you can create an \"egrep.py\" system——one that searches for patterns or regular\n",
    "# expressions in a specified location——like so:\n",
    "# <sys.argv> is the list of command-line arguments (literally just the code).\n",
    "# <sys.argv[0]> is the name of the program in-question.\n",
    "# <sys.argv[1]> is the regular expression (regex) specified at the command line.\n",
    "import sys, re\n",
    "regex = sys.argv[1]\n",
    "for line in sys.stdin:          # For every line of text passed into this program,\n",
    "    if re.search(regex, line):  # if it matches a regular expression,\n",
    "        sys.stdout.write(line)  # spit it back out!\n",
    "\n",
    "# Here's one that counts the lines recieved and writes out the count (much like\n",
    "# \"line_count.py\").\n",
    "import sys\n",
    "count = 0\n",
    "for line in sys.stdin:\n",
    "    count += 1\n",
    "    print(count)\n",
    "    \n",
    "# Or count how many lines of a file contain numbers:\n",
    "type SomeFile.txt | python egrep.py \"[0-9]\" | line_count.py\n",
    "# The | is a pipe character, meaning \"use the output of the left command as the\n",
    "# input of the right command\".\n",
    "\n",
    "# Similarly, this script counts the words in its input and writes the most common\n",
    "# (\"most_common_words.py\").\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "try:\n",
    "    num_words = int(sys.argv[1]) # \"Try\" number of words as first argument.\n",
    "except:\n",
    "    print(\"usage: most_common_words.py num_words\")\n",
    "    sys.exit(1) # (Nonzero exit code would indicate an error)\n",
    "\n",
    "counter = Counter(word.lower() # Lowercase words.\n",
    "                  for line in sys.stdin\n",
    "                  for word in line.strip().split() # Split on spaces\n",
    "                  if word) # Skip empty \"words\".\n",
    "\n",
    "for word, count in counter.most_common(num_words):\n",
    "    sys.stdout.write(str(count))\n",
    "    sys.stdout.write(\"\\t\")\n",
    "    sys.stdout.write(word)\n",
    "    sys.stdout.write(\"\\n\")\n",
    "\n",
    "$ type the_bible.txt | python most_common_words.py 10 # Which would produce:\n",
    "# 359330 the\n",
    "# 321943 and\n",
    "# .... and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30cdce-e987-4b97-8ae2-09db44489280",
   "metadata": {},
   "source": [
    "# Reading Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583a6634-586f-4e0e-a8ac-539fe9451870",
   "metadata": {},
   "source": [
    "THE BASICS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac720f-e00b-4bb1-bf17-1321b7e1da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'r' means \"read-only\", this is the assumed command if you leave one out:\n",
    "file_for_reading = open('reading_file.txt', 'r')\n",
    "\n",
    "# 'w' means write, it'll destroy the file if it already exists!\n",
    "file_for_writing = open('writing_file.txt', 'w')\n",
    "\n",
    "# 'a' is append, for adding to the end of the file:\n",
    "file_for_appending = open('appending_file.txt', 'a')\n",
    "\n",
    "# CLOSE your files when done:\n",
    "file_for_writing.close()\n",
    "\n",
    "# ...just so you don't forget to do so, use the above commands inside a\n",
    "# <with> prompt so it closes automatically:\n",
    "with open(filename) as f:\n",
    "    data = function_that_gets_data_from(f)\n",
    "process(data)\n",
    "\n",
    "# If you need to read a whole text file, iterate over it using <for>:\n",
    "starts_with_hash = 0\n",
    "with open('input.txt') as f:\n",
    "    for line in f:                 # Looks at each line in the file.\n",
    "        if re.match(\"^#\", line):   # Uses a regex to see if starts with #.\n",
    "            starts_with_hash += 1  # If it does, add 1 to the count.\n",
    "\n",
    "# You'll often want to <strip> the text into pieces. For example, you want\n",
    "# to see the number of certain domains in a list of email adresses.\n",
    "def get_domain(email_address: str) -> str:\n",
    "    \"\"\"Split on '@' and return the last piece.\"\"\"\n",
    "    return email_adress.lower().split(\"@\")[-1]\n",
    "\n",
    "assert get_domain('234242@glenbrook225.org') == 'glenbrook225.org'\n",
    "assert get_domain('c.love.00029@gmail.com') == 'gmail.com'\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "with open('email_adresses.txt', 'r') as f:\n",
    "    domain_counts = Counter(get_domain(line.strip())\n",
    "                            for line in f\n",
    "                            if \"a\" in line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66590eb-5597-4a97-ae68-1bd699c84b77",
   "metadata": {},
   "source": [
    "DELIMITED FILES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaecc9c5-ca88-4043-9c5d-e107a6360526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The previous example assumed you'd have one email per line, which is\n",
    "# optimistic. To separate a chunk of text in a file, use <csv>. With \n",
    "# tab-delimited text (two columns with a space in-between):\n",
    "import csv\n",
    "with open('tab_delimited_stock_prices.txt') as f:\n",
    "    tab_reader = csv.reader(f, delimiter = 't\\)\n",
    "    for row in tab_reader:\n",
    "        date = row[0]\n",
    "        symbol = row[1]\n",
    "        closing_price = float(row[2]) # <float> is type annotation.\n",
    "        process(date, symbol, closing_price) # See the TB for example data.\n",
    "\n",
    "# Or with headers——dict at beginning for all data——everything in rows).\n",
    "with open('colon_delimited_stock_prices.txt') as f:\n",
    "    colon_reader = csv.DictReader(f, delimiter= ':')\n",
    "    for dict_row in dict_row colon_reader:\n",
    "        date = dict_row[\"date\"]\n",
    "        symbol = dict_row[\"symbol\"]\n",
    "        closing_price = float(dict_row[\"closing_price\"])\n",
    "        process(date, symbol, closing_price) # Again, see TB for context.\n",
    "\n",
    "# You can also write out delimited data with <csv.writer>:\n",
    "\n",
    "todays_prices = {'AAPL': 90.91, 'MSFT': 41.68}\n",
    "with open('comma_delimited_stock_prices.txt', 'w') as f:\n",
    "    csv_writer = csv.writer(f, delimiter = ',')\n",
    "    for stock, price in todays_prices.items():\n",
    "        csv_writer.writerow([stock, price])\n",
    "\n",
    "# Given your og text has commas in it, all the data should write out well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792616d9-b5f4-4193-aefb-2689ea9a84bf",
   "metadata": {},
   "source": [
    "# Scraping the Web:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b888339-219e-45c3-a6f8-d9503c62fea5",
   "metadata": {},
   "source": [
    "HTMLS AND PARSING THROUGH THEM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7017440b-c3ea-4522-8bde-3bc3eb57fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using python's built-in HTTP-request commands, install\n",
    "# \"Beautiful Soup library\" and the <html5lib> parser. To stage an HTML:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = (\"https:// ... .html\")\n",
    "text = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'hmtl5lib')\n",
    "\n",
    "# Now, you can use <tag> objects, which coorespond with the tags representing the\n",
    "# structure on an HTML page. For example, to find the first <p> tag (paragraph):\n",
    "first_paragraph = soup.find('p')\n",
    "\n",
    "# Now get its text:\n",
    "first_paragraph_text = soup.p.text\n",
    "first_paragraph_words = soup.p.text.split()\n",
    "\n",
    "# Extract its attributes:\n",
    "first_paragraph_id = soup.p['id']       # Raises KeyError if no 'id'.\n",
    "first_paragraph_id2 = soup.p.get('id')  # Returns \"None\" if no 'id'.\n",
    "\n",
    "# Get multiple paragraphs:\n",
    "all_paras = soup.find_all('p') # Or, just soup('p').\n",
    "paras_w_ids = [p for p in soup('p') if p.get('id')]\n",
    "\n",
    "# To find tags with a specific class:\n",
    "important_paras = soup('p', {'class' : 'important'})\n",
    "important_paras2 = soup('p', 'important')\n",
    "important_paras3 = [p for p in soup('p')\n",
    "                    if 'important' in p.get('class', [])]\n",
    "\n",
    "# To find every element, say <span>, inside another, say <div>:\n",
    "spans_inside_divs = [span\n",
    "                     for div in soup('div')\n",
    "                     for spain in div('span')]\n",
    "# But be warned, if there one <span> sits within multiple <divs>, it'll return\n",
    "# more than once, despite only being only element."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc5ec1a-efc4-4d52-82f7-5724fd8b3326",
   "metadata": {},
   "source": [
    "Go to the book for an excellent, excellent example of how to\n",
    "use all of these to iterate over all congresspeoples' press releases for mentions of \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0624e411-c86d-4fd3-93e9-ded6bec57694",
   "metadata": {},
   "source": [
    "# Using APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f790aa-b237-4c26-88ea-3614ada4b79e",
   "metadata": {},
   "source": [
    "Application programming interfaces allow you to request data in a structured format, saving you much of the trouble of above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1f4a05-be28-44b6-a485-f77d93eb19d7",
   "metadata": {},
   "source": [
    "JSON AND XML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd032e21-63a3-402d-bdf0-e3d78fe34575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP is a protocol for transferring text, so you need to \"serialize\" it into\n",
    "# a string format, which often uses \"JavaScript Object Notation\" (JSON), which\n",
    "# are essentially dicts:\n",
    "{ \"title\" : \"Data Science Book\",\n",
    "  \"author\" : \"Joel Grus\",\n",
    "  \"publicationYear\" : 2019,\n",
    "  \"topics\" : [ \"data\", \"science\", \"data science\" ] }\n",
    "\n",
    "import json\n",
    "serialized = \"\"\"{ \"title\" : \"Data Science Book\",\n",
    "                  \"author\" : \"Joel Grus\",\n",
    "                  \"publicationYear\" : 2019,\n",
    "                  \"topics\" : [ \"data\", \"science\", \"data science\" ] }\"\"\"\n",
    "\n",
    "# \"Parse\" the JSON to create a dict using the <load> function:\n",
    "deserialized = json.loads(serialized)\n",
    "assert deserialized[\"publicationYear\"] == 2019\n",
    "assert \"data science\" in deserialized[\"topics\"]\n",
    "\n",
    "# \"Sometimes an API provider hates you, and only provides responses in XML\".\n",
    "<Book>\n",
    "  <Title>Data Science Book</Title>\n",
    "  <Author>Joel Grus</Author> # ...and so on, so use BeautifulSoup to parse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae7931d-d4e5-4f06-a488-90fdf1ebd269",
   "metadata": {},
   "source": [
    "USING AN UNAUTHENTICATED API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c846563c-401b-43fc-91e5-15c838d36bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As will be explained below, most APIs these days require authentication.\n",
    "# To start, here's Github's API and what you can do with it unauthenticated:\n",
    "import requests, json\n",
    "github_user = \"ConnorLove\"\n",
    "endpoint = f\"https://api.github.com/users/{github_user}/repos\"\n",
    "repos = json.loads(requests.get(endpoint).text)\n",
    "\n",
    "# At this point repos is a list of dicts, which we can use, say, to figure\n",
    "# our which days of the week you're most likely to create a repo:\n",
    "\"created_at\" : \"2013-07-05T02:02:28Z\"\n",
    "\n",
    "python -m pip install python-dateutil\n",
    "from collections import Counter\n",
    "from dateutil.parser import parse\n",
    "\n",
    "dates = [parse(repo[\"created_at\"]) for repo in repos] # ...and so on.\n",
    "\n",
    "# Or get the languages of the last five repos:\n",
    "\n",
    "last_5_repos = sorted(repos,\n",
    "                      key=lambda r: r[\"pushed_at\"],\n",
    "                      reverse = True) [:5]\n",
    "\n",
    "last_5_langs = [repo[\"language\"]\n",
    "                for repo in last_5_repos]\n",
    "\n",
    "# You don't need to know most of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16891c9b-f706-4721-abef-67fc8f89c2d8",
   "metadata": {},
   "source": [
    "FINDING APIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61600daf-44b0-402d-905f-6a76dd1029c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need data from a specific site, look for a \"developers\" or \"API\"\n",
    "# section of the site, and do a general internet search for \"python\n",
    "# <sitename> API\" for the appropriate library. Or you could... scrape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a69cc04-c333-40f9-8327-5403bc04a716",
   "metadata": {},
   "source": [
    "AN EXAMPLE USING TWITTER APIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14acd57-6f2a-4e2e-af62-77906bc40049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, <python -m pip install twython> (install the twitter API library).\n",
    "# Second, follow all his instructions for authenticating a developer \n",
    "# account with the API and secret API keys (always the trickiest part). \n",
    "# This is done so develops know you aren't wasting data.\n",
    "# Following the rest of the steps in the book which I'm too lazy to \n",
    "# include, however also which are not very relevant to your education."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b881be-ec0d-4d1e-ada4-e497c6708721",
   "metadata": {},
   "source": [
    "# CHAPTER 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e5367a-7fcf-4952-8d34-915335394b83",
   "metadata": {},
   "source": [
    "# Exploring Your Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d374a-4652-45a6-a958-37e51f7172f9",
   "metadata": {},
   "source": [
    "One-Dimenstional Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e7abb-5ebd-45e3-9284-3fc516ecbb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a series of numbers, you can find mean/median/mode (easy),\n",
    "# or create a histogram by dropping your data into distinct \"buckets\":\n",
    "from typing import List, Dict\n",
    "from collections import Counter\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def bucketize(point: float, bucket_size: float) -> float:\n",
    "    \"\"\"Floor the point to the next lower multiple of bucket_size.\"\"\"\n",
    "    return bucket_size * math.floor(point / bucket_size)\n",
    "    # Returns size of bucket and point divided by size of bucket?\n",
    "\n",
    "def make_histogram(points: List[float], bucket_size: float) -> Dict[float, int]:\n",
    "    \"\"\"Buckets the points and counts how many in each bucket.\"\"\"\n",
    "    return Counter(bucketize(point, bucket_size) for point in points)\n",
    "    # (See above)\n",
    "\n",
    "def plot_histogram(points: List[float], bucket_size: float, title: str = \"\"):\n",
    "    histogram = make_histogram(points, bucket_size)\n",
    "    plt.bar(histogram.keys(), histogram.values(), width = bucket_size)\n",
    "    plt.title(title)\n",
    "    # Idk it makes a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7db6d-f4fb-4a44-9153-825b17fc028e",
   "metadata": {},
   "source": [
    "Two Dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b44e253-946f-4405-92da-f47edc832335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say you had two sets of ys for the same xs, eg: \n",
    "\n",
    "def random_normal() -> float:\n",
    "    \"\"\"Return a random draw from standard normal dist.\"\"\"\n",
    "    return inverse_normal_cdf(random.random())\n",
    "\n",
    "xs = [random_normal() for _ in range(1000)]\n",
    "ys1 = [x + random_normal() / 2 for x in xs]\n",
    "ys1 = [-x + random_normal() / 2 for x in xs]\n",
    "\n",
    "# If you plotted these (see that page), they'd have same xs, diff y correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747765f2-29a3-4aef-8472-2eb3e6a63381",
   "metadata": {},
   "source": [
    "Many Dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb8dfc3-8f78-4353-8397-fea64fc02b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Um, you can use a correlation matrix, which I still sort of don't\n",
    "# understand, but believe it displays the correlations between, say,\n",
    "# x / y / z, and when a variable's column meets another's row, that's\n",
    "# their correlation, represented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc98607-b8c4-4947-b9dd-b6e2188de046",
   "metadata": {},
   "source": [
    "# Using NamedTuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf369f-fe84-4d48-ac8d-3d9f5ffd516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a lot of data, plugging it into Dicts over, and over, and over\n",
    "# is prone to errors. Instead, use \"NamedTuples\", which will create suggestions\n",
    "# in your editor when you're plugging in data (and you'll also save storage):\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "StockPrice = namedtuple('StockPrice', ['symbol', 'date', 'closing_price'])\n",
    "price = StockPrice('MSFT', datetime.date(2018, 12, 14), 106.03)\n",
    "\n",
    "assert price.symbol == 'MSFT' # T.A, because it's the right thing to do!\n",
    "assert price.closing_price # See above!\n",
    "\n",
    "# Great! But I want to type annotate, because it's the right thing to do:\n",
    "\n",
    "from typing import Namedtuple\n",
    "\n",
    "class StockPrice(NamedTuple):\n",
    "    symbol: str\n",
    "    date: datetime.date\n",
    "    closing_price: float\n",
    "    # (Ab)use the fact this is a class to create methods, remember, those\n",
    "    # things you can use to define the class at large, not just individuals?\n",
    "\n",
    "    def is_high_tech(self) -> bool:\n",
    "        \"\"\"It's a class, so we can add methods too!\"\"\"\n",
    "        return self.symbol in ['MSFT', 'GOOG', 'FB', 'AMZN', 'AAPL']\n",
    "\n",
    "price = StockPrice('MSFT', datetime.date(2018, 12, 14), 106.03)\n",
    "\n",
    "# Blah blah blah, assert it all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729104bf-914d-4348-ad03-4b37ddb3e0f9",
   "metadata": {},
   "source": [
    "# Dataclasses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7584384-658f-4ef0-88de-9ff4f27f08b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are essentially NamedTuple's bastard cousin whose instance methods\n",
    "# you can modify (again, instance methods are individual variables'\n",
    "# alterations of the symbol, date, closing_price).\n",
    "\n",
    "@dataclass # That decorator is the only change:\n",
    "class StockPrice(NamedTuple):\n",
    "    symbol: str\n",
    "    date: datetime.date\n",
    "    closing_price: float\n",
    "    # (Ab)use the fact this is a class to create methods, remember, those\n",
    "    # things you can use to define the class at large, not just individuals?\n",
    "\n",
    "    def is_high_tech(self) -> bool:\n",
    "        \"\"\"It's a class, so we can add methods too!\"\"\"\n",
    "        return self.symbol in ['MSFT', 'GOOG', 'FB', 'AMZN', 'AAPL']\n",
    "\n",
    "price = StockPrice('MSFT', datetime.date(2018, 12, 14), 106.03)\n",
    "\n",
    "price2.closing_price /= 2 # Stock split\n",
    "\n",
    "# But, with the ability to alter dataclasses, comes the very same error we\n",
    "# avoided with NamedTuples:\n",
    "\n",
    "price2.cosing_price = 75 # Damn, \"cosing\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b2763b-7920-4fef-97ba-70895ecf41da",
   "metadata": {},
   "source": [
    "# Cleaning and Munging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b32ce8-632f-46c9-bbc8-a3bdec510e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your data may be dirt, clean it like so:\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "def parse_row(row: List[str]) -> StockPrice:\n",
    "    symbol, date, closing_price = row\n",
    "    return StockPrice(symbol = symbol,\n",
    "                      date = parse(date).date(),\n",
    "                      closing_price = float(closing_price))\n",
    "\n",
    "stock = parse_row([\"MSFT\", \"2018-12-14\", \"106.03\"])\n",
    "\n",
    "# (Assertions, yada yada yada)*.\n",
    "# Now, let's test it out:\n",
    "\n",
    "from typing import Optional\n",
    "import re\n",
    "\n",
    "def try_parse_row(row: List[str]) -> Optional[StockPrice]:\n",
    "    symbol, date_, closing_price_ = row # The extra _'s are just for spacing.\n",
    "    if not re.match(r\"^[A-Z]+$\", symbol): # \"symbol\" at end just means that's\n",
    "        return None                       # the variable type it is.\n",
    "\n",
    "    try:\n",
    "        date = parse(date_).date()\n",
    "    except ValueError                     # See exceptions, chapter two,\n",
    "        return None                       # for the method.\n",
    "\n",
    "    try:\n",
    "        closing_price = float(closing_price_)\n",
    "    except Value Error\n",
    "        return None\n",
    "\n",
    "    return StockPrice(symbol, date, closing_price)\n",
    "\n",
    "# (*), except if you try to assert:\n",
    "assert try_parse_row([\"MSFT0\", \"2020-12-14\", \"106.03\"]) is None # !!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a8288-5450-47a7-bfe0-a543212e847d",
   "metadata": {},
   "source": [
    "# Manipulating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040105e0-73ec-4276-8f6c-1ae29ce56f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = [\n",
    "    StockPrice(symbol = 'MSFT',\n",
    "               date = datetime.date(2018, 12, 24),\n",
    "               closing_price = 106.03),\n",
    "    # ... and so on, including all of Apple's, and all entries generally.\n",
    "\n",
    "# Let's manipulate, starting with an example of grabbing Apple's highest-ever\n",
    "# closing price. Think about this logically, like you were a program.\n",
    "# 1.) Restrict yourself to Apple rows.\n",
    "# 2.) Grab \"closing prices\" from each row.\n",
    "# 3.) Grab the \"max\" from each row.\n",
    "\n",
    "max_appl_price = max(stock_price.closing_price\n",
    "                     for stock_price in date\n",
    "                     if stock_price.symbol == \"APPL\") # ...easy enough.\n",
    "\n",
    "# But what if we wished for the highest closing price of each stock? Logic:\n",
    "# 1.) Create a dict to keep track of the higest values, and their stocks.\n",
    "# 2.) Iterate over your data, updating it every day:\n",
    "\n",
    "from collections import deafultdict\n",
    "\n",
    "max_prices: Dict[str, float] = defaultdict(lamba: float('inf'))\n",
    "\n",
    "# for stockprice\n",
    "for sp in data:\n",
    "    symbol, closing_price = sp.symbol, sp.closing_price\n",
    "    if closing_price > max_prices[symbol]:\n",
    "        max_prices[symbol] = closing_price\n",
    "\n",
    "# But what are the largest and smallest one-day percent-shifts in all stocks?\n",
    "# 1.) Order the prices by date.\n",
    "# 2.) Zip together pairs (previous, current).\n",
    "# 3.) Convert zipped pairs into \"percent change\" rows.\n",
    "\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "\n",
    "# 0.) Group the prices by symbol.\n",
    "prices: Dict[str, List[StockPrice]] = defaultdict(list)\n",
    "\n",
    "# Add stock price to the symbol, creating the new variable symbol_prices.\n",
    "for sp in data:\n",
    "    prices[sp.symbol].append(sp)\n",
    "\n",
    "# Now that we have a bunch of prices with same symbol, sort them by date\n",
    "# instead, so they're sequential.\n",
    "\n",
    "prices = {symbol: sorted(symbol_prices)\n",
    "          for symbol, symbol_prices in prices.items()}\n",
    "\n",
    "# Now define pct-change:\n",
    "\n",
    "def pct_change(yesterday: StockPrice, Today: StockPrice) -> float:\n",
    "    return today.closing_price / yesterday.closing_price - 1 # The equation.\n",
    "\n",
    "class DailyChange(NamedTuple):\n",
    "    symbol: str\n",
    "    date: datetime.date\n",
    "    pct_change: float\n",
    "\n",
    "def day_over_day_changes(prices: List[StockPrice]) -> List[DailyChange]:\n",
    "    \"\"\"Assumes prices are for one stock and are in order.\"\"\"\n",
    "    return [DailyChange(symbol = today.symbol,\n",
    "                        date = today.date,\n",
    "                        pct_change = pct_change(yesterday, today))]\n",
    "    for yesterday, today in zip(prices, prices[1:])]\n",
    "\n",
    "# Now collect the pct-changes created by your function!\n",
    "\n",
    "all_changes = [change\n",
    "               for symbol_prices in prices.values()\n",
    "               for change in day_over_day_changes(symbol_prices)]\n",
    "\n",
    "# Now find your max change for a given day:\n",
    "max_changes = max(all_changes, key = lambda change: change.pct_change)\n",
    "assert max_change.symbol == 'APPL'\n",
    "assert max_change.date == date.date(1997, 8, 6)\n",
    "assert 0.33 < max_change.pct_change < 0.34      # Assertions just in case!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eb53ec-5ead-40ea-b004-e1568c269421",
   "metadata": {},
   "source": [
    "# Rescaling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c483f3a-3248-43b2-a158-27c7cf588f5e",
   "metadata": {},
   "source": [
    "\"TQDM\"s, or \"progress\" bars, show how long a computation is taking.\n",
    "They don't even need their own section, so just reference the book's\n",
    "\"An Aside: tqdm\" section if you'd like to include them in your work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa5dfb0-cda9-4ca9-afae-235e2025f243",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355f9da7-1d08-411e-9709-9628c2ee2be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes, the dimensions of a dataset are all wack because they're not\n",
    "# actually on the y/x-axis, beginning with 0, 0. Fear not, here's how we\n",
    "# go about fixing this / interpreting it:\n",
    "# 1.) Translate data so each dimensions has a mean of 0. \n",
    "\n",
    "from scratch.linear_algebra import subtract\n",
    "def de_mean(data: List[Vector]) -> List[Vector]:\n",
    "    \"\"\"Re-centers the data so it has a mean of 0 in every direction,\n",
    "    effectively a 'properly'-square graph!\"\"\"\n",
    "    mean = vector_mean(data)\n",
    "    return [subtract(vector, mean) for vector in data]\n",
    "\n",
    "# 2.) \"Computer, which direction captures the greatest variance in data?\"\n",
    "# (Specifically, given a direction \"d\" = magnitude of 1, every point is\n",
    "# labeled as a vector \"w\" with magnitude:\n",
    "\n",
    "from scratch.linear_algebra import magnitude\n",
    "def direction(w: Vector) -> Vector:\n",
    "    mag = magnitude(w)\n",
    "    return [w_i / mag for w_i in w]\n",
    "\n",
    "# Now use all those mini-w's to compute a general-graph variance\n",
    "# (with a big funny arrow):\n",
    "\n",
    "from scratch.linear_algebra import dot\n",
    "def directional_variance(data: List[Vector], w: Vector) -> float:\n",
    "    \"\"\"Returns the variance of x in the direction of w.\"\"\"\n",
    "    w_dir = direction(w)\n",
    "    return sum(dot(v, w_dir ** 2 for v in data))\n",
    "\n",
    "# Just read the rest of this section! It's mostly coding! You get a\n",
    "# big arrow at the end! I'm almost certainly missing a key element\n",
    "# called the \"principal component\", but I most certainly don't care."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89033e5a",
   "metadata": {},
   "source": [
    "# CHAPTER 11:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691fcfea",
   "metadata": {},
   "source": [
    "Models = Formulae (take in inputs, produce outputs).\n",
    "\n",
    "Machine Learning = Models based on data, that might be able to predict new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a67cc4",
   "metadata": {},
   "source": [
    "Supervised Models = Computer can check predictions against actually correct answers.\n",
    "\n",
    "Unsupervised = None supplied.\n",
    "\n",
    "Semisupervised = Some data labeled as correct.\n",
    "\n",
    "Online = Model continuously adjusts to new, incoming data.\n",
    "\n",
    "Reinforcemnet = After model makes certain number of predictions, gets score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2cb13b",
   "metadata": {},
   "source": [
    "Because you could theoretically use any model for any dataset (like you could use any graph for any dataset), we'll make an assumption that one (eg; decision tree, linear function) describes it best, then make the best version of one of those."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d27061b",
   "metadata": {},
   "source": [
    "Underfitting = Not a good model\n",
    "\n",
    "Overfitting = Only good for one set of data, bad for others (eg; uses inputs from that dataset for model, rather than predicting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657601dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid this, you split up your data into thirds, 2/3 used to train the\n",
    "# data, the remaining 1/3 tested upon:\n",
    "\n",
    "import random\n",
    "from typing import TypeVar, List, Tuple\n",
    "X = TypeVar('X') # Generic type to rep data point (input)\n",
    "\n",
    "def split_data(data):\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:] # Make a copy!\n",
    "    random.shuffle(data) # ...because shuffle modifies ur list\n",
    "    cut = int(len(data) * prob) # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:] # And split the list there\n",
    "\n",
    "data = [n for n in range(1000)]\n",
    "train, test = split_data(data, 0.75)\n",
    "\n",
    "# Proportions should be correct:\n",
    "assert len(train) == 750\n",
    "assert len(test) ==250\n",
    "\n",
    "# And o.g data should be preserved somehow\n",
    "assert sorted(train + test) == data\n",
    "\n",
    "# And you'll likely have input variables which correlate with outputs.\n",
    "# So you'll need to put them together in either the training or test:\n",
    "\n",
    "Y = TypeVar('Y') # Generic type to rep data point (input)\n",
    "\n",
    "def train_test_split(xs, ys, test_pct):\n",
    "    # Generate indices and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_Date(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i]] for i in train idxs, # x_train\n",
    "            .....) # You've essentially \"tagged\" each x with its y.\n",
    "\n",
    "# After which you can do something like:\n",
    "\n",
    "model = SomeKindOfModel()\n",
    "x_train, x_test, y_train, y_test = train_test_split(xs, ys, 0.33)\n",
    "model.train(x_train, y_train)\n",
    "performance = model.test(x_test, y_test)\n",
    "\n",
    "# This is pretty good, and if a model performs well on the training set\n",
    "# there's a good chance it works on the test set.\n",
    "\n",
    "# However, you could still run into the problem of the model simply\n",
    "# IDENTIFYING variables in this set, rather than discovering relationships\n",
    "# between ATTRIBUTES... especially if you have recurring variables (say,\n",
    "# users' daily data usage).\n",
    "\n",
    "# More problematically, you might've just found a model that works well\n",
    "# on both, and have effectively just run two training sessions, as opposed\n",
    "# to how it would perform on any other larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba6a5f",
   "metadata": {},
   "source": [
    "Correctness:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa2651f",
   "metadata": {},
   "source": [
    "- True positive = \"The message is spam, and we correctly predicted spam.\"\n",
    "- False positive = \"The message is not spam, but we predicted spam.\"\n",
    "- True negative = \"The message is not spam, and we correctly predicted not spam.\"\n",
    "- False negative = \"The message is spam, but we predicted not spam.\" \n",
    "\n",
    "Take a hypothesis that kids named \"Luke\" will have Leukimia. You can calculate the accuracy (correct/total) of this hypothesis with a TP/FP/TN/FN table, and:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01d6f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(tp, fp, fn, tn):\n",
    "    correct = tp + tn\n",
    "    total = tp + fp + fn + tn\n",
    "    return correct / total # Returns 0.98114... very impressive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5550bfe",
   "metadata": {},
   "source": [
    "Precision = How accurate positive predictions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64707cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp, fp, fn, tn):\n",
    "    return tp / (tp + fp) # Returns 0.014... less impressive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91316ebc",
   "metadata": {},
   "source": [
    "Recall = How many positives did we identify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03e70513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(tp, fp, fn, tn):\n",
    "    return tp / (tp + fn) # Returns 0.005... even worse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b577672",
   "metadata": {},
   "source": [
    "F1 Score = Precision plus recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4c2d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(tp, fp, fn, tn):\n",
    "    p = precision(tp, fp, fn, tn)\n",
    "    r = recall(tp, fp, fn, tn)\n",
    "    return 2 * p * r / (p + r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366029c4",
   "metadata": {},
   "source": [
    "This is otherwise known as the \"Harmonic mean\" of precision and recall, lying between them. Usually, all models involve tradeoff. A model that predicts “yes” when it’s even a little bit confident will probably have a high recall but a low precision; a model that predicts “yes” only when it’s extremely confident is likely to have a low recall and a high precision.\n",
    "\n",
    "This applies to leukemia, bc if you are less confident more people will not be included in doctor's confirmation of having leukemia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f52e6d",
   "metadata": {},
   "source": [
    "The Bias-Variance Tradeoff:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199f6dfc",
   "metadata": {},
   "source": [
    "High Bias / Low Variance = Bad model for your dataset, but would be equally bad for all datasets (...underfitting)\n",
    "- Add more features!\n",
    "\n",
    "Low Bias / High Variance = Perfect model for your dataset, but would completely change for others (...overfitting)\n",
    "- Remove features\n",
    "- Give it more data to work through!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddcbb11",
   "metadata": {},
   "source": [
    "Features = Inputs to the model (ex; spam detector)\n",
    "- Can be yes/no (eg; does the email contain word 'viagra')\n",
    "- Can be quantative (eg; how many times does 'd' appear)\n",
    "- Can be qualatative (eg; what was domain of the sender)\n",
    "\n",
    "- ...which follow these categories:\n",
    "    - Naives Bayes classifier (yes or no)\n",
    "    - Regression models (use numbers, dummy variables, 0s and 1s)\n",
    "    - Decision trees (numerical AND categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0b9664",
   "metadata": {},
   "source": [
    "# CHAPTER 12:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c503d",
   "metadata": {},
   "source": [
    "Nearest neighbors = Looking at closest resembling data in a dataset to make assumptions about a data point (eg; you leave in Northbrook, your literal closest neighbors are Dems, you are assumed to be a dem.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa559d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In real data, you'd have labels which categorize data, helpful\n",
    "# for our purposes! If you wanted to create a nearest neighbor,\n",
    "# you'd pick a number \"K\", find \"K\" nearest points in one single\n",
    "# label, then produce new data point with label in the \"middle\":\n",
    "\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "\n",
    "def raw_majority_vote(labels):\n",
    "    votes = Counter(labels)\n",
    "    winner, _ = votes.most_common(1)[0]\n",
    "    return winner\n",
    "\n",
    "# In data set of ['a', 'b', 'c', 'b'], would return 'b'.\n",
    "\n",
    "# What if you have tie?:\n",
    "\n",
    "def majority_vote(labels):\n",
    "    \"\"\"Assumed that labels ordered nearest --> farthest.\"\"\"\n",
    "    vote_counts = Counter(labels)\n",
    "    winner, winner_count = vote_counts.most_common(1)[0]\n",
    "    num_winners = len([count\n",
    "                       for count in vote_counts.values()\n",
    "                       if count == winner_count])\n",
    "    if num_winners == 1:\n",
    "        return winner # If there's only one most common\n",
    "    else:\n",
    "        return majority_vote(labels[:-1]) # Try again w/out farthest\n",
    "    \n",
    "# In data set of ['a', 'b', 'c', 'b', 'a'], would return 'b'.\n",
    "\n",
    "# You can create a classifier so the data IS, in fact, ordered:\n",
    "\n",
    "from typing import NamedTuple\n",
    "from scratch.linear_algebra import Vector, distance\n",
    "\n",
    "class LabeledPoint(NamedTuple):\n",
    "    point: Vector\n",
    "    label: str\n",
    "\n",
    "def knn_classify(k, labeled_points, new_point):\n",
    "    # Order labeled points nearest --> farthest\n",
    "    by_distance = sorted(labeled_points,\n",
    "                         key = lambda lp: distance(lp.point, new_point))\n",
    "    # Find labels for \"K\" closest\n",
    "    k_nearest_labels = [lp.label for lp in by_distance[:k]]\n",
    "    # Let those \"K\" closest \"vote\" on nearest neighbor\n",
    "    return majority_vote(k_nearest_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1749d2",
   "metadata": {},
   "source": [
    "Ex; \"The Iris Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af67d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sepal_length, sepal_width, petal_length, petal_width, class\n",
    "# ex; (5.1, 3.5, 1.4, 0.2, Iris-setosa)\n",
    "\n",
    "# Turn all the rows into LabeledPoints:\n",
    "\n",
    "def typing import Dict\n",
    "import csv # (flower file)\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_iris_row(row):\n",
    "    measurements = [float(value) for value in row[:-1]]\n",
    "    # Class is \"Iris-setosa, we only want Setosa\"\n",
    "    label = row[-1].split(\"-\")[-1]\n",
    "    return LabeledPoint(measurements, label)\n",
    "\n",
    "with open('iris.data') as f:\n",
    "    reader = csv.reader(f)\n",
    "    iris_data = [parse_iris_row(row) for row in reader]\n",
    "\n",
    "points_by_species: Dict = defaultdict\n",
    "for iris in iris_data:\n",
    "    points_by_species[iris.label].append(iris.point)\n",
    "\n",
    "# He then proceeds to make a bunch of cool plots that I don't\n",
    "# want to type out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab3366e",
   "metadata": {},
   "source": [
    "The Curse of Dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b25993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In higher dimensions (eg; with more variables to describe\n",
    "# data), your data's gonna space out more, and won't be as useful\n",
    "# for finding nearest neighbors.\n",
    "\n",
    "# Just as good practice, let's generate some points and calculate\n",
    "# the average distances between them:\n",
    "\n",
    "def random_point(dim):\n",
    "    return [random.random() for _ in range(dim)]\n",
    "\n",
    "def random_distances(dim, num_pairs):\n",
    "    return [distance(random_point(dim), random_point(dim))\n",
    "            for _ in range(num_pairs)]\n",
    "\n",
    "import tqdm\n",
    "dimensions = range(1, 101) # Range 1 --> 100 (remember takes one off)\n",
    "\n",
    "avg_distance = []\n",
    "min_distance = []\n",
    "\n",
    "random.seed(0)\n",
    "for dim in tqdm.tqdm(dimensions, desc = \"Curse of Dimensionality\"):\n",
    "    distances = random_distances(dim, 10000) # 10000 random pairs\n",
    "    avg_distance.append(sum(distances) / 10000) # Track the avg.\n",
    "    min_distance.append(min(distances)) # Track the min\n",
    "\n",
    "# As the num dimensions incr, the avg distance b/t points incr.\n",
    "# Closest pts aren’t much closer than average, so 2 pts being close\n",
    "# don't mean shit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc5dffe",
   "metadata": {},
   "source": [
    "# CHAPTER 14:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e44522",
   "metadata": {},
   "source": [
    "So let's say you've tested and come up with a good linear model to show relationship between x and y... you'll need to determine WHY it is that way:\n",
    "\n",
    "y = Beta(x) + alpha + funky lookin' e\n",
    "\n",
    "y = predicted output\n",
    "\n",
    "Beta = constant multiplied by input\n",
    "\n",
    "Alpha = y-intercept\n",
    "\n",
    "The \"e\" = Margin of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict (alpha, beta):\n",
    "    return beta * x_i * alpha\n",
    "\n",
    "def error(alpha, beta, x_i, y_i):\n",
    "    \"\"\"\n",
    "    The error from predicting beta * x_i + alpha,\n",
    "    when actual value is y_i\n",
    "    \"\"\"\n",
    "    return predict(alpha, beta, x) - y\n",
    "\n",
    "# But you'd like to know error over entire dataset, so you'll\n",
    "# add up ALL squared errors:\n",
    "\n",
    "from scratch.linear_algebra import Vector\n",
    "\n",
    "def sum_of_sqerrors(alpha, beta, x, y):\n",
    "    return sum(error(alpha, beta, x_i, y_i) ** 2\n",
    "               for x_i, y_i in zip(x, y))\n",
    "\n",
    "# The \"least squares solution\" is that bearing the lowest\n",
    "# sum_of_sqerrors, its error-minimizing alphas and betas can\n",
    "# be calculated as such:\n",
    "\n",
    "from typing import Tuple\n",
    "from scratch.linear_algebra import Vector\n",
    "from scratch.statistics import correlation, standard_deviation, mean\n",
    "\n",
    "def least_squares_fit(x, y):\n",
    "    \"\"\"Given two vectors x and y, find least-squares alpha/beta.\"\"\"\n",
    "    beta = correlation(x, y) * standard_deviation(y) / standard_deviation(x)\n",
    "    alpha = mean(y) - beta * mean(x)\n",
    "    return alpha, beta\n",
    "\n",
    "# Trust the math."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b4422b",
   "metadata": {},
   "source": [
    "How to do R-squared, the beefier cousin of above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f2fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scratch.statistics import de_mean\n",
    "\n",
    "def total_sum_of_squares(y):\n",
    "    \"\"\"The total squared variations of y_i's from their means\"\"\"\n",
    "    return sum(v ** 2 for v in de_mean(y))\n",
    "\n",
    "def r_squared(alpha, beta, x, y):\n",
    "    \"\"\"\n",
    "    The fraction of variation in y captured by the model,\n",
    "    equaling 1 - the fraction of variation in y NOT captured\n",
    "    by the model\n",
    "    \"\"\"\n",
    "    return 1.0 - (sum_of_sqerrors(alpha, beta, x, y) /\n",
    "                  total_sum_of_squares(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef4a2a1",
   "metadata": {},
   "source": [
    "Gradient Descent Strikes Back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we know theta = [alpha, beta], we could also compute\n",
    "# the model fit using gradient descent:\n",
    "\n",
    "import random\n",
    "import tqdm\n",
    "from scratch.gradient_descent import gradient_step\n",
    "\n",
    "num_epochs = 10000\n",
    "random.seed(0)\n",
    "\n",
    "guess = [random.random(), random.random()] # Choose rand\n",
    "                                           # value to start.\n",
    "learning_rate = 0.00001 # Gradient step\n",
    "\n",
    "with tqdm.trange(num_epochs) as t:\n",
    "    for _ in t:\n",
    "        alpha, beta = guess\n",
    "\n",
    "        # Partial derivative of loss with respect to alpha\n",
    "        grad_a = sum(2 * error(alpha, beta, x_i, y_i)\n",
    "                     for x_i, y_i in zip(num_friends_good\n",
    "                                         daily_minutes_good))\n",
    "        # This is in the context of a problem correlating\n",
    "        # minutes spent on a site with friends.\n",
    "\n",
    "        # Partial derivative of loss with respect to beta\n",
    "        grad_b = sum(2 * error(alpha, beta, x_i, y_i)\n",
    "                     for x_i, y_i in zip(num_friends_good\n",
    "                                         daily_minutes_good))\n",
    "        \n",
    "        # Compute los to plug into the tqdm description\n",
    "        loss = sum_of_sqerrors(alpha, beta,\n",
    "                               num_friends_good, daily_minutes_good)\n",
    "        t.set_description(f\"loss: {loss:.3f}\")\n",
    "\n",
    "        # Update the guess after computation:\n",
    "        guess = gradient_step(guess, [grad_a, grad_b], -learning_rate)\n",
    "\n",
    "# Which should produce pretty much the same R^2 !! For more work :D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f567fe",
   "metadata": {},
   "source": [
    "Maximum Likelihood Estimation:\n",
    "\n",
    "Imagine, instead of computing the probability of distribution based on an unknown theta, you computed the likelihood of that distribution based on a known theta. The most likely theta would then become that which maximizes the likelihood of your data spread occuring exactly as it does.\n",
    "\n",
    "- Another thing that exists is the \"likelihood based on an entire dataset is the product of the individual likelihood\" ... okay!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843ec5c5",
   "metadata": {},
   "source": [
    "# CHAPTER 25:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd8f26e",
   "metadata": {},
   "source": [
    "I'm boutta show you the best way to process big data:\n",
    "\n",
    "1. Use a \"mapper\" to turn each item into a key/value pair\n",
    "2. Collect all pairs with identical keys\n",
    "3. Use a \"reducer\" on each collection of ^ assoc value, produce output values for each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Count:\n",
    "\n",
    "# Here, the keys are words, and for each word we'll \n",
    "from typing import Iterator, Tuple\n",
    "def wc_mapper(document):\n",
    "    \"\"\"For each word in the document, produce (word, 1)\"\"\"\n",
    "    for word in tokenize(document):\n",
    "        yield (word, 1)\n",
    "\n",
    "from typing import Iterable\n",
    "def wc_reducer(word, counts):\n",
    "    yield (word, sum(counts))\n",
    "\n",
    "from collections import defaultdict\n",
    "def word_count(documents):\n",
    "    \"\"\"Count the words in the input documents using MapReduce\"\"\"\n",
    "    collector = defaultdict(list) # To store grouped values.\n",
    "\n",
    "    for document in documents:\n",
    "        for word, count in wc_mapper(document):\n",
    "            collector[word].append(count)\n",
    "\n",
    "    return [output \n",
    "            for word, counts in collector.items()\n",
    "            for output in wc_reducer(word, counts)]\n",
    "\n",
    "# If we had three documents \"data science\", \"big data\", \"science fiction\",\n",
    "# the collector would contain\n",
    "\n",
    "{\"data\" : [1, 1],\n",
    " \"science\" : [1, 1],\n",
    " \"big\" : [1],\n",
    " \"fiction\" : [1]}\n",
    "\n",
    "# And wc_reducer would produce the counts for each word:\n",
    "\n",
    "[(\"data\", 2), (\"science\", 2), (\"big\", 1), (\"fiction\", 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a138cbd2",
   "metadata": {},
   "source": [
    "It good because it fast, trust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805250e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To put this simply in generic terms:\n",
    "\n",
    "from typing import Callable, Iterable, Any, Tuple\n",
    "\n",
    "# A key/value pair is just a tuple:\n",
    "KV = Tuple[Any, Any]\n",
    "\n",
    "# A Mapper is a Callable which \"activates\" the iterable for a specific key:\n",
    "Mapper = Callable[..., Iterable[KV]]\n",
    "\n",
    "# A Reducer is a function that takes a key and an interable of values\n",
    "# and returns a key/value pair:\n",
    "Reducer = Callable[[Any, Iterable], KV]\n",
    "\n",
    "# 1.) Mapper gives any input a key\n",
    "# 2.) Reducer combines all inputs with that key into a single clump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787be092",
   "metadata": {},
   "source": [
    "ex;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257756e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_science_day_mapper(status_update):\n",
    "    \"\"\"Yields (day_of_week, 1) if status_update contains \"data science\" \"\"\"\n",
    "    if \"data science\" in status_update[\"text\"].lower():\n",
    "        day_of_week = status_update[\"created_at\"].weekday()\n",
    "        yield (day_of_week, 1)\n",
    "\n",
    "data_science_days = map_reduce(status_updates,\n",
    "                               data_science_day_mapper,\n",
    "                               sum_reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3135ed",
   "metadata": {},
   "source": [
    "ex;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_per_user_mapper(status_update):\n",
    "    user = status_update[\"username\"]\n",
    "    for word in tokenize(status_update[\"text\"]):\n",
    "        yield (user, (word, 1))\n",
    "\n",
    "def most_popular_word_reducer(user):\n",
    "    \"\"\"\n",
    "    Given a sequence of (word, count) pairs,\n",
    "    return the word with the highest total count.\n",
    "    \"\"\"\n",
    "    word_counts = Counter()\n",
    "    for word, count in words_and_counts:\n",
    "        word_counts[word] += count\n",
    "\n",
    "    word, count = word_counts.most_common(1)[0]\n",
    "\n",
    "    yield (user, (word, count))\n",
    "\n",
    "user_words = map_reduce(status_updates,\n",
    "                        words_per_user_mapper,\n",
    "                        most_popular_word_reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e6435",
   "metadata": {},
   "source": [
    "Matrix Multiplication:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b94c60d",
   "metadata": {},
   "source": [
    "I am too tired for this I'm so sorry if this comes up on a test. Just read it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92abba95",
   "metadata": {},
   "source": [
    "Combiners:\n",
    "\n",
    "- The reason you don't use (word, None) and then just take the length of all (as opposed to what we do) is because sometimes you have one machine designated for mapping, and another for reduction.\n",
    "- Trust :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
